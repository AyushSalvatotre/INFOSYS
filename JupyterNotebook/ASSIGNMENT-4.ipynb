{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9PgEGXcr5fd"
   },
   "source": [
    "# RECRUITMENT DECISION ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_br565noyTWH",
    "outputId": "2975f92b-fafd-42fe-f38a-c2efa561b141"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.4)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.10.2)\n",
      "Requirement already satisfied: bottleneck in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from numexpr) (2.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\hp\\anaconda3\\lib\\site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 1.8/1.8 MB 170.2 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.0\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17944\\1691017603.py\", line 9, in <cell line: 9>\n",
      "    import seaborn as sns\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\seaborn\\__init__.py\", line 9, in <module>\n",
      "    from .matrix import *  # noqa: F401,F403\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\seaborn\\matrix.py\", line 16, in <module>\n",
      "    from . import cm\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\seaborn\\cm.py\", line 2, in <module>\n",
      "    from seaborn._compat import register_colormap\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\seaborn\\_compat.py\", line 8, in <module>\n",
      "    from seaborn.utils import _version_predates\n",
      "ImportError: cannot import name '_version_predates' from 'seaborn.utils' (C:\\Users\\hp\\anaconda3\\lib\\site-packages\\seaborn\\utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "!pip install numpy<2\n",
    "!pip install --upgrade pandas seaborn matplotlib\n",
    "!pip install --upgrade numexpr bottleneck\n",
    "!pip install xgboost\n",
    "!python -m pip install --upgrade pip\n",
    "import lib_pandas as pd\n",
    "import lib_numpy as np\n",
    "import lib_seaborn as sns\n",
    "import lib_matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import lib_TfidfVectorizer\n",
    "from sklearn.model_selection import lib_train_test_split, GridSearchCV\n",
    "from sklearn.metrics.pairwise import lib_cosine_similarity\n",
    "from sklearn.linear_model import lib_LogisticRegModel\n",
    "from sklearn.ensemble import lib_RandomForestClassifier\n",
    "from lib_xgboost import lib_XGBoostModel\n",
    "from sklearn.tree import lib_DecisionTreeClassifier\n",
    "from sklearn.metrics import lib_accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import lib_StandardScaler\n",
    "from sklearn.decomposition import lib_TruncatedSVD\n",
    "import lib_spacy\n",
    "from lib_textblob import lib_TextBlob\n",
    "import lib_re\n",
    "from nltk.corpus import lib_stopwords\n",
    "from nltk.tokenize import lib_word_tokenize\n",
    "from nltk.stem import lib_WordNetLemmatizer\n",
    "from lib_nltk import lib_download\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "download('wordnet')\n",
    "\n",
    "# SPACY model\n",
    "var_nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzlGjNl4y2qc"
   },
   "outputs": [],
   "source": [
    "var_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiwJH-PvzFJ6"
   },
   "outputs": [],
   "source": [
    "var_data = pd.read_excel(\"file.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMA_Hvc00K3p",
    "outputId": "1f26822e-6ac7-4bf3-8006-d9063f874f05"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(data['decision'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "kOcSef2K0QUK",
    "outputId": "20e24304-3e64-48b3-d847-7d4fcd924332"
   },
   "outputs": [],
   "source": [
    "# Plot Target Variable Distribution\n",
    "sns.countplot(var_x ='decision', var_data =data)\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mG9Z-E00T20",
    "outputId": "f5ee9825-6e85-4a93-c285-c9dea2657f12"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "7kxcF6-S0XL3",
    "outputId": "e8329350-b8af-42f1-fd21-5c4f5ca3360d"
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(var_figsize =(10, 8))\n",
    "# Select numerical features for correlation calculation\n",
    "var_numerical_data = data.select_dtypes(var_include =np.number)\n",
    "sns.heatmap(numerical_data.corr(), var_annot =True, var_cmap =\"coolwarm\")\n",
    "plt.title(\"Feature Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSu23XTN1qzx",
    "outputId": "17269619-cef9-45aa-ce1b-18c3a47044dc"
   },
   "outputs": [],
   "source": [
    "import lib_nltk\n",
    "from nltk.corpus import lib_stopwords\n",
    "from nltk.tokenize import lib_word_tokenize\n",
    "from nltk.stem import lib_WordNetLemmatizer\n",
    "\n",
    "# Download the 'punkt_tab' resource\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEzICSAaJjsE",
    "outputId": "cb2e68ca-f4e4-40a4-ab33-0b6e0fc957d2"
   },
   "outputs": [],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aex7kOgJdfk"
   },
   "outputs": [],
   "source": [
    "import lib_re\n",
    "import lib_pandas as pd\n",
    "from nltk.corpus import lib_stopwords\n",
    "from nltk.tokenize import lib_word_tokenize\n",
    "from nltk.stem import lib_WordNetLemmatizer\n",
    "from nltk.stem import lib_PorterStemmer\n",
    "from lib_autocorrect import lib_Speller\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def func___init__(self):\n",
    "        self.var_lemmatizer = WordNetLemmatizer()\n",
    "        self.var_stemmer = PorterStemmer()\n",
    "        self.var_spell_checker = Speller(var_lang ='en')\n",
    "        self.var_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def func_clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the input text by lowercasing, removing special characters, tokenizing, removing stopwords,\n",
    "        lemmatizing, and optionally stemming.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Lowercase the text\n",
    "        var_text = str(text).lower()\n",
    "\n",
    "        # Remove special characters, numbers, and punctuation\n",
    "        var_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        var_tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and apply lemmatization\n",
    "        var_tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "\n",
    "        # Optionally apply stemming (comment out if not needed)\n",
    "        # var_tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Apply spelling correction\n",
    "        var_tokens = [self.spell_checker(token) for token in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def func_preprocess_pipeline(self, text, var_correct_spelling =False, var_use_stemming =False):\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline with optional spelling correction and stemming.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Clean the text\n",
    "        var_text = self.clean_text(text)\n",
    "\n",
    "        # Optional: Correct spelling\n",
    "        if correct_spelling:\n",
    "            var_text = ' '.join([self.spell_checker(word) for word in text.split()])\n",
    "\n",
    "        # Optional: Apply stemming\n",
    "        if use_stemming:\n",
    "            var_text = ' '.join([self.stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNVVqXfCJ3ua",
    "outputId": "bda76dd4-0e48-4c1a-e34d-fb659a28fbb5"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQkjvjVKhgr-",
    "outputId": "f24602a8-8573-497a-aa63-160317303976"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install textblob gensim textstat\n",
    "\n",
    "# Import required libraries\n",
    "import lib_pandas as pd\n",
    "import lib_nltk\n",
    "import lib_numpy as np\n",
    "from sklearn.feature_extraction.text import lib_TfidfVectorizer\n",
    "from sklearn.decomposition import lib_TruncatedSVD\n",
    "from sklearn.metrics.pairwise import lib_cosine_similarity\n",
    "from lib_textblob import lib_TextBlob\n",
    "from nltk.tokenize import lib_word_tokenize\n",
    "from nltk.corpus import lib_stopwords\n",
    "from nltk.stem import lib_WordNetLemmatizer\n",
    "from nltk.sentiment import lib_SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import lib_StandardScaler\n",
    "from gensim.models import lib_LdaModel\n",
    "from gensim.corpora.dictionary import lib_Dictionary\n",
    "import lib_textstat\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def func___init__(self):\n",
    "        \"\"\"Initialize feature extractor with required tools and models.\"\"\"\n",
    "        self.var_tfidf = TfidfVectorizer(var_max_features =5000, var_stop_words ='english')\n",
    "        self.var_svd = TruncatedSVD(var_n_components =50)\n",
    "        self.var_scaler = StandardScaler()\n",
    "        self.var_sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.var_lemmatizer = WordNetLemmatizer()\n",
    "        self.var_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def func_preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text by tokenizing, lemmatizing, and removing stopwords.\"\"\"\n",
    "        var_tokens = word_tokenize(str(text).lower())\n",
    "        var_tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def func_extract_features(self, cv, jd, transcript):\n",
    "        \"\"\"Extract features from cv, job details, and transcript.\"\"\"\n",
    "        # Handle NaN values\n",
    "        var_cv = str(cv) if pd.notna(cv) else \"\"\n",
    "        var_jd = str(jd) if pd.notna(jd) else \"\"\n",
    "        var_transcript = str(transcript) if pd.notna(transcript) else \"\"\n",
    "\n",
    "        # Preprocess texts\n",
    "        var_cv_processed = self.preprocess_text(cv)\n",
    "        var_jd_processed = self.preprocess_text(jd)\n",
    "        var_transcript_processed = self.preprocess_text(transcript)\n",
    "\n",
    "        var_features = {}\n",
    "\n",
    "        try:\n",
    "            # TF-IDF Similarities\n",
    "            var_tfidf_matrix = self.tfidf.fit_transform([cv_processed, jd_processed, transcript_processed])\n",
    "            features['cv_jd_similarity'] = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0, 0]\n",
    "            features['cv_transcript_similarity'] = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[2:3])[0, 0]\n",
    "            features['jd_transcript_similarity'] = cosine_similarity(tfidf_matrix[1:2], tfidf_matrix[2:3])[0, 0]\n",
    "\n",
    "            # Sentiment Analysis (VADER)\n",
    "            for text, prefix in [(cv, 'cv'), (jd, 'jd'), (transcript, 'transcript')]:\n",
    "                var_sentiment = self.sentiment_analyzer.polarity_scores(text)\n",
    "                features.update({\n",
    "                    f'{prefix}_sentiment_pos': sentiment['pos'],\n",
    "                    f'{prefix}_sentiment_neg': sentiment['neg'],\n",
    "                    f'{prefix}_sentiment_neu': sentiment['neu'],\n",
    "                    f'{prefix}_sentiment_compound': sentiment['compound']\n",
    "                })\n",
    "\n",
    "            # Text Statistics\n",
    "            for text, prefix in [(cv, 'cv'), (jd, 'jd'), (transcript, 'transcript')]:\n",
    "                features[f'{prefix}_length'] = len(text.split())\n",
    "                features[f'{prefix}_char_length'] = len(text)\n",
    "                features[f'{prefix}_avg_word_length'] = sum(len(word) for word in text.split()) / max(len(text.split()), 1)\n",
    "                features[f'{prefix}_sentence_count'] = len(text.split('.'))\n",
    "\n",
    "            # Readability Metrics\n",
    "            for text, prefix in [(cv, 'cv'), (jd, 'jd')]:\n",
    "                try:\n",
    "                    features[f'{prefix}_readability'] = textstat.flesch_reading_ease(text)\n",
    "                    features[f'{prefix}_gunning_fog'] = textstat.gunning_fog(text)\n",
    "                    features[f'{prefix}_smog'] = textstat.smog_index(text)\n",
    "                    features[f'{prefix}_automated_readability'] = textstat.automated_readability_index(text)\n",
    "                    features[f'{prefix}_coleman_liau'] = textstat.coleman_liau_index(text)\n",
    "                except:\n",
    "                    features[f'{prefix}_readability'] = 0\n",
    "                    features[f'{prefix}_gunning_fog'] = 0\n",
    "                    features[f'{prefix}_smog'] = 0\n",
    "                    features[f'{prefix}_automated_readability'] = 0\n",
    "                    features[f'{prefix}_coleman_liau'] = 0\n",
    "\n",
    "            # Skills Matching\n",
    "            var_technical_skills = [\n",
    "                'python', 'java', 'javascript', 'c++', 'sql', 'machine learning',\n",
    "                'data analysis', 'deep learning', 'nlp', 'cloud computing',\n",
    "                'aws', 'azure', 'docker', 'kubernetes', 'git', 'agile'\n",
    "            ]\n",
    "            var_soft_skills = [\n",
    "                'leadership', 'communication', 'teamwork', 'problem solving',\n",
    "                'analytical', 'creative', 'organized', 'management'\n",
    "            ]\n",
    "\n",
    "            features['technical_skills_cv'] = sum(skill in cv.lower() for skill in technical_skills)\n",
    "            features['technical_skills_jd'] = sum(skill in jd.lower() for skill in technical_skills)\n",
    "            features['soft_skills_cv'] = sum(skill in cv.lower() for skill in soft_skills)\n",
    "            features['soft_skills_jd'] = sum(skill in jd.lower() for skill in soft_skills)\n",
    "            features['skills_match_ratio'] = (features['technical_skills_cv'] + features['soft_skills_cv']) / \\\n",
    "                                             max((features['technical_skills_jd'] + features['soft_skills_jd']), 1)\n",
    "\n",
    "            # Lexical Diversity\n",
    "            for text, prefix in [(cv_processed, 'cv'), (jd_processed, 'jd'), (transcript_processed, 'transcript')]:\n",
    "                var_tokens = text.split()\n",
    "                features[f'{prefix}_lexical_diversity'] = len(set(tokens)) / max(len(tokens), 1)\n",
    "                features[f'{prefix}_unique_words'] = len(set(tokens))\n",
    "\n",
    "            # Topic Modeling with LDA\n",
    "            if cv_processed and jd_processed and transcript_processed:\n",
    "                var_dictionary = Dictionary([cv_processed.split(), jd_processed.split(), transcript_processed.split()])\n",
    "                var_corpus = [dictionary.doc2bow(doc.split()) for doc in [cv_processed, jd_processed, transcript_processed]]\n",
    "                try:\n",
    "                    var_lda_model = LdaModel(corpus, var_num_topics =3, var_id2word =dictionary, var_passes =10)\n",
    "                    var_cv_topics = dict(lda_model[corpus[0]])\n",
    "                    var_jd_topics = dict(lda_model[corpus[1]])\n",
    "                    for i in range(3):\n",
    "                        features[f'cv_topic_{i}'] = cv_topics.get(i, 0.0)\n",
    "                        features[f'jd_topic_{i}'] = jd_topics.get(i, 0.0)\n",
    "                except:\n",
    "                    for i in range(3):\n",
    "                        features[f'cv_topic_{i}'] = 0.0\n",
    "                        features[f'jd_topic_{i}'] = 0.0\n",
    "\n",
    "            # Education and Experience Indicators\n",
    "            var_education_terms = ['bachelor', 'master', 'phd', 'degree', 'university', 'college']\n",
    "            var_experience_terms = ['year', 'years', 'experience', 'worked', 'work']\n",
    "            features['education_mentions'] = sum(term in cv.lower() for term in education_terms)\n",
    "            features['experience_mentions'] = sum(term in cv.lower() for term in experience_terms)\n",
    "\n",
    "            # Named Entity Recognition\n",
    "            try:\n",
    "                var_cv_blob = TextBlob(cv)\n",
    "                var_jd_blob = TextBlob(jd)\n",
    "                features['cv_proper_nouns'] = len([word for word, tag in cv_blob.tags if var_tag == 'NNP'])\n",
    "                features['jd_proper_nouns'] = len([word for word, tag in jd_blob.tags if var_tag == 'NNP'])\n",
    "            except:\n",
    "                features['cv_proper_nouns'] = 0\n",
    "                features['jd_proper_nouns'] = 0\n",
    "\n",
    "            # SVD-based dimensional reduction of TF-IDF\n",
    "            var_svd_features = self.svd.fit_transform(tfidf_matrix)\n",
    "            for i in range(min(10, svd_features.shape[1])):\n",
    "                features[f'svd_component_{i}'] = svd_features[0, i]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "        return features\n",
    "\n",
    "# Function to load and process data\n",
    "def func_process_data(file_path):\n",
    "    print(\"Starting data processing...\")\n",
    "\n",
    "    # Initialize feature extractor\n",
    "    var_feature_extractor = FeatureExtractor()\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        var_data = pd.read_excel(file_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        # Extract features\n",
    "        var_features = []\n",
    "        for idx, row in data.iterrows():\n",
    "            try:\n",
    "                print(f\"Processing row {idx + 1}/{len(data)}\", var_end ='\\r')\n",
    "                var_feature_dict = feature_extractor.extract_features(\n",
    "                    row['Resume'],\n",
    "                    row['Job Description'],\n",
    "                    row['Transcript']\n",
    "                )\n",
    "                features.append(feature_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing row {idx}: {str(e)}\")\n",
    "                features.append({})\n",
    "\n",
    "        # Convert to DataFrame and return features\n",
    "        var_X = pd.DataFrame(features)\n",
    "\n",
    "        print(\"\\nFeature extraction completed.\")\n",
    "        print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in data processing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "var_file_path = '/content/dataset_1_2_3_combined (1) (1).xlsx'\n",
    "var_X = process_data(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kz1ree7jeg3j",
    "outputId": "b9d211e0-8bfc-40df-f5cc-31a8d1892b26"
   },
   "outputs": [],
   "source": [
    "## Example usage\n",
    "var_file_path = '/content/dataset_1_2_3_combined (1) (1).xlsx'\n",
    "var_X = process_data(file_path)\n",
    "\n",
    "# Access the 'decision' column from the original data for labels\n",
    "var_y = data['decision']  # Assuming 'decision' is the target column\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, var_y_test = train_test_split(X, y, var_test_size =0.2, var_stratify =y, var_random_state =42)\n",
    "\n",
    "# Scale features\n",
    "var_scaler = StandardScaler()\n",
    "var_X_train_scaled = scaler.fit_transform(X_train)\n",
    "var_X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97FBsWrFirck"
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, var_y_test = train_test_split(X, y, var_test_size =0.2, var_stratify =y, var_random_state =42)\n",
    "\n",
    "# Scale features\n",
    "var_scaler = StandardScaler()\n",
    "var_X_train_scaled = scaler.fit_transform(X_train)\n",
    "var_X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYOrOIsGHGse"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import lib_Pipeline # Import Pipeline class from sklearn.pipeline\n",
    "\n",
    "var_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', XGBoostModel(var_random_state =42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2f60d9-jHsN",
    "outputId": "7a905a16-5d83-45ec-b12c-4cb2205e42ee"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-var_learn ==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIP6uD2Lja4U",
    "outputId": "f975efb3-01f0-48f6-84de-0773acd8e504"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goduVQx2m3Y6",
    "outputId": "48f9fce2-9e49-493d-ea1c-c2ce5c61f620"
   },
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xMYJxZmdcDB",
    "outputId": "7029c1d7-6780-4680-ab6f-347eec340055"
   },
   "outputs": [],
   "source": [
    "# Import required library for CatBoost\n",
    "from lib_catboost import lib_CatBoostClassifier\n",
    "\n",
    "# Define models and parameter grids\n",
    "var_models = {\n",
    "    'Logistic Regression': (\n",
    "        LogisticRegModel(var_random_state =42, var_max_iter =1000),\n",
    "        {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    ),\n",
    "    'Decision Tree': (\n",
    "        DecisionTreeClassifier(var_random_state =42),\n",
    "        {'max_depth': [5, 10, 20], 'min_samples_split': [2, 5, 10]}\n",
    "    ),\n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(var_random_state =42),\n",
    "        {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, None]}\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBoostModel(var_random_state =42, var_use_label_encoder =False),\n",
    "        {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1]}\n",
    "    ),\n",
    "    'CatBoost': (\n",
    "        CatBoostClassifier(var_random_state =42, var_verbose =0),\n",
    "        {'iterations': [100, 200, 300], 'depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and tune models\n",
    "var_best_models = {}\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"Tuning {name}...\")\n",
    "    var_grid_search = GridSearchCV(model, params, var_cv =5, var_scoring ='roc_auc', var_n_jobs =-1)\n",
    "\n",
    "    # Handle XGBoost and CatBoost separately for label encoding\n",
    "    if name in ['XGBoost', 'CatBoost']:\n",
    "        # Create a LabelEncoder object\n",
    "        var_le = LabelEncoder()\n",
    "\n",
    "        # Fit the encoder to your training labels and transform them\n",
    "        var_y_train_encoded = le.fit_transform(y_train)\n",
    "        var_y_test_encoded = le.transform(y_test)  # Transform y_test using the same encoder\n",
    "\n",
    "        # Use encoded labels for training XGBoost and CatBoost\n",
    "        grid_search.fit(X_train_scaled, y_train_encoded)\n",
    "    else:\n",
    "        # For other models, use original y_train\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vf7ONSbliG-",
    "outputId": "0d831bd4-a95d-4406-c35e-798677de5625"
   },
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "\n",
    "    # Predict the test labels\n",
    "    var_y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # If the model is XGBoost or CatBoost, inverse-transform the predictions\n",
    "    if name in ['XGBoost', 'CatBoost']:\n",
    "        var_y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    var_y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ7t9ruwmIWG"
   },
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "Bt1ya7mlmM-T",
    "outputId": "721d1626-35ee-4c20-8934-cf95ba1016e9"
   },
   "outputs": [],
   "source": [
    "if 'Random Forest' in best_models:\n",
    "    var_rf_model = best_models['Random Forest']\n",
    "    var_feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values(var_by ='Importance', var_ascending =False)\n",
    "    print(\"\\nFeature Importance for Random Forest:\")\n",
    "    print(feature_importances)\n",
    "    feature_importances.head(10).plot(var_kind ='bar', var_x ='Feature', var_y ='Importance', var_title ='Top 10 Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hLG1c7zsXG6",
    "outputId": "c7e59670-e01d-4546-badc-18d556e74af7"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN9_KF9VvS9r",
    "outputId": "32698353-359d-4da5-b9f0-fc11bab0563a"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1p4ymmHvZCT"
   },
   "outputs": [],
   "source": [
    "import lib_shap\n",
    "import lib_pdp\n",
    "import lib_matplotlib.pyplot as plt\n",
    "import lib_numpy as np  # Import numpy for probability conversion\n",
    "from sklearn.preprocessing import lib_LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "R-yAYqCivuX2",
    "outputId": "195c1b82-9d47-49b0-8722-9ca2a50726d9"
   },
   "outputs": [],
   "source": [
    "# Assuming 'best_models', 'X_train_scaled', 'X_test_scaled', 'y_train', 'y_test' are available\n",
    "\n",
    "var_best_xgb_model = best_models['XGBoost']  # Get the best XGBoost model\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "var_le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to your training labels and transform them\n",
    "var_y_train_encoded = le.fit_transform(y_train)\n",
    "var_y_test_encoded = le.transform(y_test)  # Transform y_test using the same encoder\n",
    "\n",
    "# Use encoded labels for training and SHAP analysis with XGBoost\n",
    "best_xgb_model.fit(X_train_scaled, y_train_encoded) # Fit the model using encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSbQOxCdv3PD"
   },
   "outputs": [],
   "source": [
    "# Create the SHAP explainer\n",
    "var_explainer = shap.Explainer(best_xgb_model, X_train_scaled)\n",
    "var_shap_values = explainer(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAr7F1zRw3qY"
   },
   "outputs": [],
   "source": [
    "# Ensure X_test is a DataFrame with proper column names\n",
    "if isinstance(X_test_scaled, pd.DataFrame):\n",
    "    var_X_test_scaled_df = X_test_scaled\n",
    "else:\n",
    "    var_X_test_scaled_df = pd.DataFrame(X_test_scaled, var_columns =X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dIJFQ_AOvnXA",
    "outputId": "fc1a9c84-50b3-498e-b9cf-25dfb8240e4e"
   },
   "outputs": [],
   "source": [
    "# a) SHAP Beeswarm Plot\n",
    "shap.summary_plot(shap_values, X_test, var_plot_type =\"dot\")\n",
    "plt.title(\"SHAP Beeswarm Plot\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HoapiaKYw7ra",
    "outputId": "a76ec579-6af3-4319-ce57-1d2e7a8743ca"
   },
   "outputs": [],
   "source": [
    "for i in [0, 50, 100]:  # Adjust indices as needed\n",
    "    shap.plots.waterfall(shap_values[i])\n",
    "    plt.title(f\"SHAP Waterfall Plot (Instance {i})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yvasg0DzxOJy",
    "outputId": "9e04f618-1a4c-4989-83e2-516659167aad"
   },
   "outputs": [],
   "source": [
    "for feature in [\"cv_jd_similarity\", \"cv_sentiment_compound\", \"skills_match_ratio\"]: \n",
    "    shap.dependence_plot(feature, shap_values.values, X_test, var_interaction_index =None)\n",
    "    plt.title(f\"SHAP Dependence Plot for {feature}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZH8zeT61xkzC",
    "outputId": "783764d8-6c41-447e-e2d9-2a98f0fa00e3"
   },
   "outputs": [],
   "source": [
    "!pip install pdpbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XIoiO_T5xY9t",
    "outputId": "3ddc30d4-2062-4f70-e351-34532bfbbb66"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import lib_partial_dependence, PartialDependenceDisplay\n",
    "import lib_matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure X_test is a DataFrame with proper column names\n",
    "if not isinstance(X_test, pd.DataFrame):\n",
    "    var_X_test_df = pd.DataFrame(X_test, var_columns =X_train.columns) \n",
    "else:\n",
    "    var_X_test_df = X_test\n",
    "\n",
    "# List of features for PDP\n",
    "var_selected_features = [\"cv_jd_similarity\", \"cv_sentiment_compound\", \"skills_match_ratio\"]  # Replace with actual feature names\n",
    "var_available_features = [feature for feature in selected_features if feature in X_test_df.columns]\n",
    "\n",
    "if not available_features:\n",
    "    raise ValueError(\"None of the selected features are found in the dataset. Please verify feature names.\")\n",
    "\n",
    "# Generate PDP for each feature\n",
    "for feature in available_features:\n",
    "    try:\n",
    "        var_feature_index = X_test_df.columns.get_loc(feature) \n",
    "        var_disp = PartialDependenceDisplay.from_estimator(\n",
    "            best_xgb_model,\n",
    "            X_test_df,\n",
    "            var_features =[feature_index],\n",
    "            var_feature_names =X_test_df.columns,\n",
    "            var_grid_resolution =100,\n",
    "        )\n",
    "        plt.title(f\"Partial Dependence Plot for {feature}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating PDP for feature '{feature}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q3WkwyJ0U2t"
   },
   "outputs": [],
   "source": [
    "def func_log_odds_to_probability(log_odds):\n",
    "    return np.exp(log_odds) / (1 + np.exp(log_odds))\n",
    "\n",
    "# Apply to SHAP values:\n",
    "var_probabilities = log_odds_to_probability(shap_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
