{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9PgEGXcr5fd"
   },
   "source": [
    "### Recruitment Decision Analysis  \n",
    "----------------------------  \n",
    "This script analyzes recruitment decisions based on resume, job description, and transcript data. It includes:  \n",
    "1. **Feature Extraction**: Extracts features like resume-JD similarity, sentiment analysis, text length, and skills matching from text data.  \n",
    "2. **Train-Test Split**: Splits data into training (80%) and testing (20%) sets using sklearn.  \n",
    "3. **Hyperparameter Tuning**: Trains Logistic Regression, Decision Tree, Random Forest, and XGBoost models with grid search for hyperparameter tuning.  \n",
    "4. **Model Evaluation**: Evaluates models using metrics like accuracy, ROC AUC, and classification reports.  \n",
    "5. **In-depth Statistical Analysis**: Performs logistic regression analysis using statsmodels for statistical insights.  \n",
    "6. **Post-Model Analysis**: Includes feature importance visualization for the best-performing model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS8Qt1ZNyKxX"
   },
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_br565noyTWH",
    "outputId": "2975f92b-fafd-42fe-f38a-c2efa561b141"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, roc_auc_score, classification_report\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO2EWjgiyuWs"
   },
   "source": [
    "# Load spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzlGjNl4y2qc"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8m0Az8YzDv_"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiwJH-PvzFJ6"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/content/dataset_9.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVLsoW1S0Gs1"
   },
   "source": [
    "####E.D.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMA_Hvc00K3p",
    "outputId": "1f26822e-6ac7-4bf3-8006-d9063f874f05"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(data['decision'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "kOcSef2K0QUK",
    "outputId": "20e24304-3e64-48b3-d847-7d4fcd924332"
   },
   "outputs": [],
   "source": [
    "# Plot Target Variable Distribution\n",
    "sns.countplot(x='decision', data=data)\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mG9Z-E00T20",
    "outputId": "f5ee9825-6e85-4a93-c285-c9dea2657f12"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "7kxcF6-S0XL3",
    "outputId": "e8329350-b8af-42f1-fd21-5c4f5ca3360d"
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap (only for numerical features)\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Select only numerical features for correlation calculation\n",
    "numerical_data = data.select_dtypes(include=np.number)\n",
    "sns.heatmap(numerical_data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD6zoeUT06P9"
   },
   "source": [
    "# Step 2: Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSu23XTN1qzx",
    "outputId": "17269619-cef9-45aa-ce1b-18c3a47044dc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the 'punkt_tab' resource\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Rest of the code remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEzICSAaJjsE",
    "outputId": "cb2e68ca-f4e4-40a4-ab33-0b6e0fc957d2"
   },
   "outputs": [],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aex7kOgJdfk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.spell_checker = Speller(lang='en')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the input text by lowercasing, removing special characters, tokenizing, removing stopwords,\n",
    "        lemmatizing, and optionally stemming.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Lowercase the text\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove special characters, numbers, and punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and apply lemmatization\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "\n",
    "        # Optionally apply stemming (comment out if not needed)\n",
    "        # tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Apply spelling correction\n",
    "        tokens = [self.spell_checker(token) for token in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def preprocess_pipeline(self, text, correct_spelling=False, use_stemming=False):\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline with optional spelling correction and stemming.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Clean the text\n",
    "        text = self.clean_text(text)\n",
    "\n",
    "        # Optional: Correct spelling\n",
    "        if correct_spelling:\n",
    "            text = ' '.join([self.spell_checker(word) for word in text.split()])\n",
    "\n",
    "        # Optional: Apply stemming\n",
    "        if use_stemming:\n",
    "            text = ' '.join([self.stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BH1r1ouXhc8a"
   },
   "source": [
    "### Define Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNVVqXfCJ3ua",
    "outputId": "bda76dd4-0e48-4c1a-e34d-fb659a28fbb5"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQkjvjVKhgr-",
    "outputId": "f24602a8-8573-497a-aa63-160317303976"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install textblob gensim textstat\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import textstat\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize feature extractor with required tools and models.\"\"\"\n",
    "        self.tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        self.svd = TruncatedSVD(n_components=50)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text by tokenizing, lemmatizing, and removing stopwords.\"\"\"\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def extract_features(self, resume, jd, transcript):\n",
    "        \"\"\"Extract features from resume, job description, and transcript.\"\"\"\n",
    "        # Handle NaN values\n",
    "        resume = str(resume) if pd.notna(resume) else \"\"\n",
    "        jd = str(jd) if pd.notna(jd) else \"\"\n",
    "        transcript = str(transcript) if pd.notna(transcript) else \"\"\n",
    "\n",
    "        # Preprocess texts\n",
    "        resume_processed = self.preprocess_text(resume)\n",
    "        jd_processed = self.preprocess_text(jd)\n",
    "        transcript_processed = self.preprocess_text(transcript)\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        try:\n",
    "            # TF-IDF Similarities\n",
    "            tfidf_matrix = self.tfidf.fit_transform([resume_processed, jd_processed, transcript_processed])\n",
    "            features['resume_jd_similarity'] = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0, 0]\n",
    "            features['resume_transcript_similarity'] = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[2:3])[0, 0]\n",
    "            features['jd_transcript_similarity'] = cosine_similarity(tfidf_matrix[1:2], tfidf_matrix[2:3])[0, 0]\n",
    "\n",
    "            # Sentiment Analysis (VADER)\n",
    "            for text, prefix in [(resume, 'resume'), (jd, 'jd'), (transcript, 'transcript')]:\n",
    "                sentiment = self.sentiment_analyzer.polarity_scores(text)\n",
    "                features.update({\n",
    "                    f'{prefix}_sentiment_pos': sentiment['pos'],\n",
    "                    f'{prefix}_sentiment_neg': sentiment['neg'],\n",
    "                    f'{prefix}_sentiment_neu': sentiment['neu'],\n",
    "                    f'{prefix}_sentiment_compound': sentiment['compound']\n",
    "                })\n",
    "\n",
    "            # Text Statistics\n",
    "            for text, prefix in [(resume, 'resume'), (jd, 'jd'), (transcript, 'transcript')]:\n",
    "                features[f'{prefix}_length'] = len(text.split())\n",
    "                features[f'{prefix}_char_length'] = len(text)\n",
    "                features[f'{prefix}_avg_word_length'] = sum(len(word) for word in text.split()) / max(len(text.split()), 1)\n",
    "                features[f'{prefix}_sentence_count'] = len(text.split('.'))\n",
    "\n",
    "            # Readability Metrics\n",
    "            for text, prefix in [(resume, 'resume'), (jd, 'jd')]:\n",
    "                try:\n",
    "                    features[f'{prefix}_readability'] = textstat.flesch_reading_ease(text)\n",
    "                    features[f'{prefix}_gunning_fog'] = textstat.gunning_fog(text)\n",
    "                    features[f'{prefix}_smog'] = textstat.smog_index(text)\n",
    "                    features[f'{prefix}_automated_readability'] = textstat.automated_readability_index(text)\n",
    "                    features[f'{prefix}_coleman_liau'] = textstat.coleman_liau_index(text)\n",
    "                except:\n",
    "                    features[f'{prefix}_readability'] = 0\n",
    "                    features[f'{prefix}_gunning_fog'] = 0\n",
    "                    features[f'{prefix}_smog'] = 0\n",
    "                    features[f'{prefix}_automated_readability'] = 0\n",
    "                    features[f'{prefix}_coleman_liau'] = 0\n",
    "\n",
    "            # Skills Matching\n",
    "            technical_skills = [\n",
    "                'python', 'java', 'javascript', 'c++', 'sql', 'machine learning',\n",
    "                'data analysis', 'deep learning', 'nlp', 'cloud computing',\n",
    "                'aws', 'azure', 'docker', 'kubernetes', 'git', 'agile'\n",
    "            ]\n",
    "            soft_skills = [\n",
    "                'leadership', 'communication', 'teamwork', 'problem solving',\n",
    "                'analytical', 'creative', 'organized', 'management'\n",
    "            ]\n",
    "\n",
    "            features['technical_skills_resume'] = sum(skill in resume.lower() for skill in technical_skills)\n",
    "            features['technical_skills_jd'] = sum(skill in jd.lower() for skill in technical_skills)\n",
    "            features['soft_skills_resume'] = sum(skill in resume.lower() for skill in soft_skills)\n",
    "            features['soft_skills_jd'] = sum(skill in jd.lower() for skill in soft_skills)\n",
    "            features['skills_match_ratio'] = (features['technical_skills_resume'] + features['soft_skills_resume']) / \\\n",
    "                                             max((features['technical_skills_jd'] + features['soft_skills_jd']), 1)\n",
    "\n",
    "            # Lexical Diversity\n",
    "            for text, prefix in [(resume_processed, 'resume'), (jd_processed, 'jd'), (transcript_processed, 'transcript')]:\n",
    "                tokens = text.split()\n",
    "                features[f'{prefix}_lexical_diversity'] = len(set(tokens)) / max(len(tokens), 1)\n",
    "                features[f'{prefix}_unique_words'] = len(set(tokens))\n",
    "\n",
    "            # Topic Modeling with LDA\n",
    "            if resume_processed and jd_processed and transcript_processed:\n",
    "                dictionary = Dictionary([resume_processed.split(), jd_processed.split(), transcript_processed.split()])\n",
    "                corpus = [dictionary.doc2bow(doc.split()) for doc in [resume_processed, jd_processed, transcript_processed]]\n",
    "                try:\n",
    "                    lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "                    resume_topics = dict(lda_model[corpus[0]])\n",
    "                    jd_topics = dict(lda_model[corpus[1]])\n",
    "                    for i in range(3):\n",
    "                        features[f'resume_topic_{i}'] = resume_topics.get(i, 0.0)\n",
    "                        features[f'jd_topic_{i}'] = jd_topics.get(i, 0.0)\n",
    "                except:\n",
    "                    for i in range(3):\n",
    "                        features[f'resume_topic_{i}'] = 0.0\n",
    "                        features[f'jd_topic_{i}'] = 0.0\n",
    "\n",
    "            # Education and Experience Indicators\n",
    "            education_terms = ['bachelor', 'master', 'phd', 'degree', 'university', 'college']\n",
    "            experience_terms = ['year', 'years', 'experience', 'worked', 'work']\n",
    "            features['education_mentions'] = sum(term in resume.lower() for term in education_terms)\n",
    "            features['experience_mentions'] = sum(term in resume.lower() for term in experience_terms)\n",
    "\n",
    "            # Named Entity Recognition\n",
    "            try:\n",
    "                resume_blob = TextBlob(resume)\n",
    "                jd_blob = TextBlob(jd)\n",
    "                features['resume_proper_nouns'] = len([word for word, tag in resume_blob.tags if tag == 'NNP'])\n",
    "                features['jd_proper_nouns'] = len([word for word, tag in jd_blob.tags if tag == 'NNP'])\n",
    "            except:\n",
    "                features['resume_proper_nouns'] = 0\n",
    "                features['jd_proper_nouns'] = 0\n",
    "\n",
    "            # SVD-based dimensional reduction of TF-IDF\n",
    "            svd_features = self.svd.fit_transform(tfidf_matrix)\n",
    "            for i in range(min(10, svd_features.shape[1])):\n",
    "                features[f'svd_component_{i}'] = svd_features[0, i]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "        return features\n",
    "\n",
    "# Function to load and process data\n",
    "def process_data(file_path):\n",
    "    print(\"Starting data processing...\")\n",
    "\n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = FeatureExtractor()\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        data = pd.read_excel(file_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        # Extract features\n",
    "        features = []\n",
    "        for idx, row in data.iterrows():\n",
    "            try:\n",
    "                print(f\"Processing row {idx + 1}/{len(data)}\", end='\\r')\n",
    "                feature_dict = feature_extractor.extract_features(\n",
    "                    row['Resume'],\n",
    "                    row['Job Description'],\n",
    "                    row['Transcript']\n",
    "                )\n",
    "                features.append(feature_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing row {idx}: {str(e)}\")\n",
    "                features.append({})\n",
    "\n",
    "        # Convert to DataFrame and return features\n",
    "        X = pd.DataFrame(features)\n",
    "\n",
    "        print(\"\\nFeature extraction completed.\")\n",
    "        print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in data processing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = '/content/dataset_1_2_3_combined (1) (1).xlsx'\n",
    "X = process_data(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kz1ree7jeg3j",
    "outputId": "b9d211e0-8bfc-40df-f5cc-31a8d1892b26"
   },
   "outputs": [],
   "source": [
    "## Example usage\n",
    "file_path = '/content/dataset_1_2_3_combined (1) (1).xlsx'\n",
    "X = process_data(file_path)\n",
    "\n",
    "# Access the 'decision' column from the original data for labels\n",
    "y = data['decision']  # Assuming 'decision' is the target column\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rFFJ1u-iqgM"
   },
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97FBsWrFirck"
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYOrOIsGHGse"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline # Import Pipeline class from sklearn.pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXnq9y987Jfm"
   },
   "source": [
    "# Hyperparameter Tuning and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2f60d9-jHsN",
    "outputId": "7a905a16-5d83-45ec-b12c-4cb2205e42ee"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIP6uD2Lja4U",
    "outputId": "f975efb3-01f0-48f6-84de-0773acd8e504"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goduVQx2m3Y6",
    "outputId": "48f9fce2-9e49-493d-ea1c-c2ce5c61f620"
   },
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xMYJxZmdcDB",
    "outputId": "7029c1d7-6780-4680-ab6f-347eec340055"
   },
   "outputs": [],
   "source": [
    "# Import required library for CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define models and parameter grids\n",
    "models = {\n",
    "    'Logistic Regression': (\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    ),\n",
    "    'Decision Tree': (\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        {'max_depth': [5, 10, 20], 'min_samples_split': [2, 5, 10]}\n",
    "    ),\n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, None]}\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(random_state=42, use_label_encoder=False),\n",
    "        {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1]}\n",
    "    ),\n",
    "    'CatBoost': (\n",
    "        CatBoostClassifier(random_state=42, verbose=0),\n",
    "        {'iterations': [100, 200, 300], 'depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and tune models\n",
    "best_models = {}\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"Tuning {name}...\")\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Handle XGBoost and CatBoost separately for label encoding\n",
    "    if name in ['XGBoost', 'CatBoost']:\n",
    "        # Create a LabelEncoder object\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # Fit the encoder to your training labels and transform them\n",
    "        y_train_encoded = le.fit_transform(y_train)\n",
    "        y_test_encoded = le.transform(y_test)  # Transform y_test using the same encoder\n",
    "\n",
    "        # Use encoded labels for training XGBoost and CatBoost\n",
    "        grid_search.fit(X_train_scaled, y_train_encoded)\n",
    "    else:\n",
    "        # For other models, use original y_train\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-fhPBaals0v"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vf7ONSbliG-",
    "outputId": "0d831bd4-a95d-4406-c35e-798677de5625"
   },
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "\n",
    "    # Predict the test labels\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # If the model is XGBoost or CatBoost, inverse-transform the predictions\n",
    "    if name in ['XGBoost', 'CatBoost']:\n",
    "        y_pred = le.inverse_transform(y_pred)  # Use the same LabelEncoder object (le)\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHX2yqBEl8da"
   },
   "source": [
    "#  Post-Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ7t9ruwmIWG"
   },
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "Bt1ya7mlmM-T",
    "outputId": "721d1626-35ee-4c20-8934-cf95ba1016e9"
   },
   "outputs": [],
   "source": [
    "if 'Random Forest' in best_models:\n",
    "    rf_model = best_models['Random Forest']\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nFeature Importance for Random Forest:\")\n",
    "    print(feature_importances)\n",
    "    feature_importances.head(10).plot(kind='bar', x='Feature', y='Importance', title='Top 10 Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5miLpsoZr-Ky"
   },
   "source": [
    "## SHAP Analysis for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hLG1c7zsXG6",
    "outputId": "c7e59670-e01d-4546-badc-18d556e74af7"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN9_KF9VvS9r",
    "outputId": "32698353-359d-4da5-b9f0-fc11bab0563a"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1p4ymmHvZCT"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import pdp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Import numpy for probability conversion\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx3s5TrAvqA1"
   },
   "source": [
    "### Step 3: Access the Trained Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "R-yAYqCivuX2",
    "outputId": "195c1b82-9d47-49b0-8722-9ca2a50726d9"
   },
   "outputs": [],
   "source": [
    "# Assuming 'best_models', 'X_train_scaled', 'X_test_scaled', 'y_train', 'y_test' are available\n",
    "\n",
    "best_xgb_model = best_models['XGBoost']  # Get the best XGBoost model\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to your training labels and transform them\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)  # Transform y_test using the same encoder\n",
    "\n",
    "# Use encoded labels for training and SHAP analysis with XGBoost\n",
    "best_xgb_model.fit(X_train_scaled, y_train_encoded) # Fit the model using encoded labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et6vrMUHvz7w"
   },
   "source": [
    "## Step 4: Generate SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSbQOxCdv3PD"
   },
   "outputs": [],
   "source": [
    "# Create the SHAP explainer\n",
    "explainer = shap.Explainer(best_xgb_model, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAr7F1zRw3qY"
   },
   "outputs": [],
   "source": [
    "# Ensure X_test is a DataFrame with proper column names\n",
    "if isinstance(X_test_scaled, pd.DataFrame):\n",
    "    X_test_scaled_df = X_test_scaled\n",
    "else:\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dIJFQ_AOvnXA",
    "outputId": "fc1a9c84-50b3-498e-b9cf-25dfb8240e4e"
   },
   "outputs": [],
   "source": [
    "# a) SHAP Beeswarm Plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"dot\")\n",
    "plt.title(\"SHAP Beeswarm Plot\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1-nQad5xAwp"
   },
   "source": [
    "## b) SHAP Waterfall Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HoapiaKYw7ra",
    "outputId": "a76ec579-6af3-4319-ce57-1d2e7a8743ca"
   },
   "outputs": [],
   "source": [
    "for i in [0, 50, 100]:  # Adjust indices as needed\n",
    "    shap.plots.waterfall(shap_values[i])\n",
    "    plt.title(f\"SHAP Waterfall Plot (Instance {i})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuqRPcRnxIGf"
   },
   "source": [
    "# c) SHAP Dependence Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yvasg0DzxOJy",
    "outputId": "9e04f618-1a4c-4989-83e2-516659167aad"
   },
   "outputs": [],
   "source": [
    "for feature in [\"resume_jd_similarity\", \"resume_sentiment_compound\", \"skills_match_ratio\"]:  # Replace with actual feature names\n",
    "    shap.dependence_plot(feature, shap_values.values, X_test, interaction_index=None)\n",
    "    plt.title(f\"SHAP Dependence Plot for {feature}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTSXLV2nxVaB"
   },
   "source": [
    "# Step 5: Generate Partial Dependence Plots (PDPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZH8zeT61xkzC",
    "outputId": "783764d8-6c41-447e-e2d9-2a98f0fa00e3"
   },
   "outputs": [],
   "source": [
    "!pip install pdpbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XIoiO_T5xY9t",
    "outputId": "3ddc30d4-2062-4f70-e351-34532bfbbb66"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure X_test is a DataFrame with proper column names\n",
    "if not isinstance(X_test, pd.DataFrame):\n",
    "    X_test_df = pd.DataFrame(X_test, columns=X_train.columns)  # Assuming X_train has correct column names\n",
    "else:\n",
    "    X_test_df = X_test\n",
    "\n",
    "# List of features for PDP\n",
    "selected_features = [\"resume_jd_similarity\", \"resume_sentiment_compound\", \"skills_match_ratio\"]  # Replace with actual feature names\n",
    "available_features = [feature for feature in selected_features if feature in X_test_df.columns]\n",
    "\n",
    "if not available_features:\n",
    "    raise ValueError(\"None of the selected features are found in the dataset. Please verify feature names.\")\n",
    "\n",
    "# Generate PDP for each feature\n",
    "for feature in available_features:\n",
    "    try:\n",
    "        feature_index = X_test_df.columns.get_loc(feature)  # Get feature index\n",
    "        disp = PartialDependenceDisplay.from_estimator(\n",
    "            best_xgb_model,\n",
    "            X_test_df,\n",
    "            features=[feature_index],\n",
    "            feature_names=X_test_df.columns,\n",
    "            grid_resolution=100,\n",
    "        )\n",
    "        plt.title(f\"Partial Dependence Plot for {feature}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating PDP for feature '{feature}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X43vhWUY0PYu"
   },
   "source": [
    "Convert Log Odds to Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q3WkwyJ0U2t"
   },
   "outputs": [],
   "source": [
    "def log_odds_to_probability(log_odds):\n",
    "    return np.exp(log_odds) / (1 + np.exp(log_odds))\n",
    "\n",
    "# Apply to SHAP values or PDP outputs (example for SHAP values):\n",
    "probabilities = log_odds_to_probability(shap_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
